<!DOCTYPE html>
<html lang="en">
  <head>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link rel="stylesheet" href="style.css" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SymFormer: End-to-end symbolic regression using transformer-based architecture</title>
    <meta name="description" content="This webpage presents the SymFormer paper and additional results. SymFormer is a transformer-based symbolic regression model. The model is capable of generating a symbolic expression of an unknown function which generated observed data quickly.">
    <link href="https://fonts.googleapis.com/css2?family=Lato:wght@400;700&display=swap" rel="stylesheet">
    <script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']]
  }
};
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>
  </head>
  <body>
    <header>
      <h1>
        <span class="title-main">SymFormer</span>
        <span class="title-small">End-to-end symbolic regression using transformer-based architecture</span>
      </h1>
    </header>
    <div class="authors">
      <div class="author">
        <span class="author-name">
          <a href="https://www.linkedin.com/in/vastl-martin/">Martin Vastl</a>
        </span>
        <span class="author-affiliation">Czech Technical University in Prague</span>
      </div>
      <div class="author">
        <span class="author-name">
          <a href="https://jkulhanek.github.io/">Jonáš Kulhánek</a>
        </span>
        <span class="author-affiliation">Czech Technical University in Prague</span>
      </div>
      <div class="author">
        <span class="author-name">
          <a href="https://www.linkedin.com/in/jiri-kubalik">Jiří Kubalík</a>
        </span>
        <span class="author-affiliation">Czech Technical University in Prague</span>
      </div>
      <div class="author">
        <span class="author-name">
          <a href="http://people.ciirc.cvut.cz/~derneeri/">Erik Derner</a>
        </span>
        <span class="author-affiliation">Czech Technical University in Prague</span>
      </div>
      <div class="author">
        <span class="author-name">
          <a href="http://www.robertbabuska.com/">Robert Babuška</a>
        </span>
        <span class="author-affiliation">Delft University of Technology</span>
      </div>
    </div>
    <div class="links">
      <div class="link link-paper">
        <a href="https://arxiv.org/pdf/2205.15764">Paper</a>
      </div>
      <div class="link link-github">
        <a href="https://github.com/vastlik/symformer/">Code</a>
      </div>
    </div>
    <div class="video header-video">
      <video width="320" height="240" loop autoplay controls muted>
        <source src="https://data.ciirc.cvut.cz/public/projects/2022SymFormer/videos/intro-video-notitle.mp4" type="video/mp4">
      </video>
    </div>

    <section class="abstract">
      <h2>Abstract</h2>
      <p>
Many real-world problems can be naturally described by mathematical formulas.
Recently, neural networks have been applied to the task of finding formulas from observed data.
We propose a novel transformer-based method called <strong>SymFormer</strong> which we train on a large number of formulas (hundreds of millions).
After training our method is considerably faster than state-of-the-art evolutionary methods.
The main novelty of our approach is that SymFormer predicts the formula by outputting the individual symbols and the corresponding constants simultaneously.
      <img src="resources/overview.svg" alt="SymFormer architecture overview" style="width: 100%; margin: 1em auto 0.3em auto; display: block;" />
This leads to better performance in terms of fitting the available data than alternative transformer-based models.
In addition, the constants provided by SymFormer serve as a good starting point for subsequent tuning via gradient descent to further improve the performance.
We show on a set of benchmarks that SymFormer outperforms two state-of-the-art methods while having faster inference.
    </section>
    <section class="results-qualitative">
      <h2>Predictions on unseen data</h2>
      <p>In this section we present qualitative results generated on the testing dataset.
      </p>
<style>
:root {
  --figblue: #1f77b4;
  --figorange: #d96e0f;
}
</style>
      <figure class="plot-figure">
        <img src="resources/fig-pred-3.svg" />
        <figcaption>
          Example 1:
          <span style="color: var(--figblue)">GT</span>: $(1+x^{-2})^{-0.5}$,
          <span style="color: var(--figorange)">Pred</span>: $\sin(|\mathrm{atan}(x)|)$
        </figcaption>
      </figure>
      <figure class="plot-figure" style="overflow: hidden;">
        <img src="resources/fig-pred-5.svg" />
        <figcaption>
          Example 2:
          <span style="color: var(--figblue)">GT</span>: $-60.9 \cdot x \cdot \exp(-x)$,<br>
          <span style="color: var(--figorange)">Pred</span>: $0.002x^3 - 61.2 \cdot x \cdot \exp(-x)$
        </figcaption>
      </figure>
      <figure class="plot-figure" style="width: 100%; margin: 1em 0">
        <div style="display: flex;">
          <img style="flex: 1 1 50%;" src="resources/fig-pred-7a.svg" />
          <img style="flex: 1 1 50%;" src="resources/fig-pred-7b.svg" />
        </div>
        <figcaption>
          Example 3:
          <span style="color: var(--figblue)">GT</span>: $x-x^3+y^{-1}\sin{(y)}$,
          <span style="color: var(--figorange)">Pred</span>: $x-x^3+y^{-1}\sin{(y)}$
        </figcaption>
      </figure>
    </section>
    <section class="results-quantitative">
      <h2>Comparison to previous approaches</h2>
      <p>We have evaluated SymFormer on common benchmarks (see the paper) and compared it to current state-of-the-art approaches:
      NSRS [<a class="citation" href="#ref-biggio2021neural"/>1</a>] and DSO [<a class="citation" href="#ref-mundhenk2021symbolic">2</a>].
      </p>
      <figure class="table">
  <figcaption>
    Table 1: Results comparing SymFormer with state-of-the-art methods on several benchmarks.
We report R2 and the average time to generate an equation.
  </figcaption>
<style>
#table1 tr td:nth-child(1),
#table1 tr th:nth-child(1)
{
  text-align: left;
}
#table1 tr td:nth-child(2),
#table1 tr td:nth-child(3),
#table1 tr td:nth-child(4),
#table1 tr td:nth-child(5),
#table1 tr td:nth-child(6),
#table1 tr td:nth-child(7),
#table1 tr th:nth-child(2),
#table1 tr th:nth-child(3),
#table1 tr th:nth-child(4)
#table1 tr th:nth-child(5),
#table1 tr th:nth-child(6),
#table1 tr th:nth-child(7)
{
  text-align: center;
}
</style>
 <table id="table1">
<thead>
  <tr>
    <td></td>
    <td colspan="2"><strong>SymFormer</strong></td>
    <td colspan="2">NSRS [<a class="citation" href="#ref-biggio2021neural"/>1</a>]</td>
    <td colspan="2">DSO [<a class="citation" href="#ref-mundhenk2021symbolic">2</a>]</td>
  </tr>
<tr>
<th>benchmark</th>
<th style="font-weight: normal">R<sup>2</sup></th>
<th style="font-weight: normal">time (s)</th>
<th style="font-weight: normal">R<sup>2</sup></th>
<th style="font-weight: normal">time (s)</th>
<th style="font-weight: normal">R<sup>2</sup></th>
<th style="font-weight: normal">time (s)</th>
</tr>
</thead><tbody>
<tr><td>Nguyen</td>       <td>0.99998</td> <td>47.50</td>  <td>0.96744</td> <td>169.46</td> <td>0.99297</td> <td>140.25</td></tr>
<tr><td>R</td>            <td>0.99986</td> <td>94.33</td>  <td>1.00000</td> <td>95.67</td>  <td>0.97488</td> <td>855.33</td></tr>
<tr><td>Livermore</td>    <td>0.99996</td> <td>43.00</td>  <td>0.88551</td> <td>193.09</td> <td>0.99651</td> <td>276.32</td></tr>
<tr><td>Koza</td>         <td>1.00000</td> <td>101.00</td> <td>0.99999</td> <td>111.50</td> <td>1.00000</td> <td>217.50</td></tr>
<tr><td>Keijzer</td>      <td>0.99904</td> <td>48.67</td>  <td>0.97392</td> <td>255.50</td> <td>0.95302</td> <td>3929.50</td></tr>
<tr><td>Constant</td>     <td>0.99998</td> <td>90.88</td>  <td>0.88742</td> <td>230.38</td> <td>1.00000</td> <td>2816.19</td></tr>
<tr class="sep"><td>Overall avg.</td> <td>0.99978</td> <td>52.95</td>  <td>0.92901</td> <td>199.63</td> <td>0.99443</td> <td>326.53</td></tr>
</tbody>
 </table>
</figure>
    </section>
    <section class="references">
      <h2>References</h2>
      <div id="ref-biggio2021neural">
        [1] Biggio, L., Bendinelli, T., Neitz, A., Lucchi, A. and Parascandolo, G., 2021, July. <i>Neural Symbolic Regression that Scales</i>. In International Conference on Machine Learning, pages 936-945. PMLR.
      </div>
      <div id="ref-mundhenk2021symbolic">
        [2] Mundhenk, T.N., Landajuela, M., Glatt, R., Santiago, C.P., Faissol, D.M. and Petersen, B.K., 2021. <i>Symbolic Regression via Neural-Guided Genetic Programming Population Seeding</i>. arXiv preprint arXiv:2111.00053.
      </div>
    </section>
    <section class="citation">
      <h2>Citation</h2>
      <span>Please use the following citation:</span>
      <pre>
@article{vastl2022symformer,
  title={SymFormer: End-to-end symbolic regression using transformer-based architecture},
  author={Vastl, Martin and Kulh{\'a}nek, Jon{\'a}{\v{s}} and Kubal{\'i}k, Ji{\v{r}}{\'i} and Derner, Erik and Babu{\v{s}}ka, Robert},
  journal={arXiv preprint arXiv:2205.15764},
  year={2022},
}
</pre>
    </section>
<a href="https://github.com/vastlik/symformer/" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
  </body>
</html>
